SWUFE-BERT 大型金融语言模型将在包含 1700 万条金融新闻和机构研报的超过 200G 语料库中进行预训练和微调，构建金融领域词典，提供一套面向金融场景的端到端自然语言处理框架，并开源以辅助金融学术研究和行业应用，填补该领域的空白。
为了测试SWUFE-BERT预训练模型的效果，我们开启了篇章级抽取式自动文本摘要的下游任务实验。在此基础上，我们将 SWUFE-BERT 与 Google 原生中文 BERT、哈工大讯飞实验室开源的 RoBERTa-wwm-ext-large进行比较实验。
该实验的具体内容是对比三种BERT预训练模型在自动生成金融短讯抽取式摘要上的表现。该实验的具体过程有：准备金融短讯有监督语料1万条（5000条为混有非金融相关内容的新闻短讯、5000条为高质量金融相关的新闻短讯）、采用三种BERT模型分别对语料进行预训练（形成Word embedding）、采用LSTM和TransformerEncoder的深度学习模型进行摘要抽取的训练和预测、对比训练效果和预测结果。